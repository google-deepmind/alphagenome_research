{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# AlphaGenome Finetuning Tutorial\n",
        "\n",
        "This notebook demonstrates how to finetune an AlphaGenome model on custom\n",
        "genomic tracks.\n",
        "\n",
        "**What you'll learn:**\n",
        "\n",
        "-   How to define custom track metadata for finetuning\n",
        "-   How to set up the data pipeline for training\n",
        "-   How to initialize and configure the model with new output heads\n",
        "-   How to run the training loop with JAX/Haiku\n",
        "-   How to use the finetuned model for inference"
      ],
      "metadata": {
        "id": "pF3KFbLU43Es"
      }
    },
    {
      "metadata": {
        "id": "jf4VXgRpVpoK"
      },
      "cell_type": "markdown",
      "source": [
        "### Prerequisites\n",
        "\n",
        "Install AlphaGenome Research package."
      ]
    },
    {
      "metadata": {
        "id": "_zqLSh1KVoCw"
      },
      "cell_type": "code",
      "source": [
        "from IPython.display import clear_output\n",
        "! PIP_NO_BINARY=pyBigWig pip install git+https://github.com/google-deepmind/alphagenome_research.git\n",
        "clear_output()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Environment Setup\n",
        "\n",
        "First, we configure TensorFlow to avoid GPU conflicts since we only use it for\n",
        "data loading (JAX handles the actual training)."
      ],
      "metadata": {
        "id": "A5xvnnEq47SW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Hide local GPUs/TPUs. TensorFlow only used for data loading.\n",
        "tf.config.set_visible_devices([], 'GPU')\n",
        "tf.config.set_visible_devices([], 'TPU')"
      ],
      "metadata": {
        "id": "Ypfsezhx0XTN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Imports\n",
        "\n",
        "Import the necessary libraries:\n",
        "\n",
        "-   `alphagenome_research.finetuning` contains the finetuning utilities\n",
        "-   `alphagenome_research.model` provides the model architecture and metadata\n",
        "    handling\n",
        "-   `alphagenome.data` provides genomic data utilities"
      ],
      "metadata": {
        "id": "Vr_Pb9K2494m"
      }
    },
    {
      "metadata": {
        "id": "ABBpbXKEUEVt"
      },
      "cell_type": "code",
      "source": [
        "import dataclasses\n",
        "from datetime import datetime\n",
        "import os\n",
        "import pprint\n",
        "\n",
        "from alphagenome.data import fold_intervals\n",
        "from alphagenome.data import genome\n",
        "from alphagenome.visualization import plot_components\n",
        "from alphagenome_research.finetuning import dataset as dataset_lib\n",
        "from alphagenome_research.finetuning import finetune\n",
        "from alphagenome_research.model import dna_model\n",
        "from alphagenome_research.model.metadata import metadata as metadata_lib\n",
        "from etils import epath\n",
        "import huggingface_hub\n",
        "import haiku as hk\n",
        "import jax\n",
        "from jax.experimental import mesh_utils\n",
        "from jax.sharding import Mesh, PartitionSpec as P\n",
        "import numpy as np\n",
        "import optax\n",
        "import orbax.checkpoint as ocp\n",
        "import pandas as pd"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Configuration\n",
        "\n",
        "Define the key hyperparameters for finetuning:\n",
        "\n",
        "-   `LEARNING_RATE`: Controls the step size during optimization\n",
        "-   `MODEL_VERSION`: Which pretrained fold to use (FOLD_0 through FOLD_3)\n",
        "-   `NUM_TRAIN_STEPS`: Number of training steps for which we optimize the model.\n",
        "-   `SEQUENCE_LENGTH`: Length of input DNA sequences (1M bp = 2^20). Training\n",
        "    requires at least 2**17.\n",
        "-   `BATCH_SIZE`: Number of samples per device.\n",
        "-   `ORGANISM`: Target organism for predictions. Harded-coded to human for now."
      ],
      "metadata": {
        "id": "THpcHfch5EnZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LEARNING_RATE = 1e-3\n",
        "NUM_TRAIN_STEPS = 1000\n",
        "MODEL_VERSION = dna_model.ModelVersion.FOLD_0\n",
        "SEQUENCE_LENGTH = int(2**20)\n",
        "BATCH_SIZE = 1  # Per device\n",
        "ORGANISM = dna_model.Organism.HOMO_SAPIENS\n",
        "SAVE_CHECKPOINT_DIR = '/tmp/checkpoint'"
      ],
      "metadata": {
        "id": "6fSzqLRLyQRi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Track Metadata\n",
        "\n",
        "Define which genomic tracks to finetune on. Each track requires:\n",
        "\n",
        "-   `name`: Human-readable name.\n",
        "-   `output_type`: Output type of assay (e.g., `RNA_SEQ`, `DNASE`, `CHIP_TF`,\n",
        "    `ATAC`). One of `dna_model.OutputType`.\n",
        "-   `strand`: Strand orientation (`+`, `-`, or `.` for unstranded)\n",
        "-   `nonzero_mean`: Optional mean of non-zero values (used for normalization).\n",
        "-   `file_path`: Path to the BigWig file containing the track data."
      ],
      "metadata": {
        "id": "nyf-cDfV5aQC"
      }
    },
    {
      "metadata": {
        "id": "HHIYfk_TT6PP"
      },
      "cell_type": "code",
      "source": [
        "! pushd /tmp && curl \\\n",
        "  -C - \\\n",
        "  -Z -O https://storage.googleapis.com/alphagenome/reference/encode/hg38/ENCFF018EZY.bigWig \\\n",
        "  -O https://storage.googleapis.com/alphagenome/reference/encode/hg38/ENCFF904TSK.bigWig \\\n",
        "  -O https://storage.googleapis.com/alphagenome/reference/encode/hg38/ENCFF218CLQ.bigWig && popd\n",
        "\n",
        "TRACK_METADATA = pd.DataFrame(\n",
        "    data=[\n",
        "        [\n",
        "            'RNA_SEQ',\n",
        "            'UBERON:0000948 total RNA-seq',\n",
        "            '+',\n",
        "            '/tmp/ENCFF018EZY.bigWig',\n",
        "        ],\n",
        "        [\n",
        "            'RNA_SEQ',\n",
        "            'UBERON:0000948 total RNA-seq',\n",
        "            '-',\n",
        "            '/tmp/ENCFF904TSK.bigWig',\n",
        "        ],\n",
        "        [\n",
        "            'DNASE',\n",
        "            'EFO:0005337 DNase-seq',\n",
        "            '.',\n",
        "            '/tmp/ENCFF218CLQ.bigWig',\n",
        "        ],\n",
        "    ],\n",
        "    columns=['output_type', 'name', 'strand', 'file_path'],\n",
        ")\n",
        "TRACK_METADATA"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build Output Metadata\n",
        "\n",
        "Convert the track DataFrame into an `AlphaGenomeOutputMetadata` object that\n",
        "configures the model's output heads."
      ],
      "metadata": {
        "id": "wkdHK2Jr55qb"
      }
    },
    {
      "metadata": {
        "id": "08vIx5EMT_Yz"
      },
      "cell_type": "code",
      "source": [
        "def build_output_metadata(\n",
        "    track_metadata: pd.DataFrame,\n",
        ") -> metadata_lib.AlphaGenomeOutputMetadata:\n",
        "  \"\"\"Builds AlphaGenomeOutputMetadata from the track metadata DataFrame.\n",
        "\n",
        "  Args:\n",
        "    track_metadata: A pandas DataFrame containing metadata for the tracks,\n",
        "      including 'output_type', 'name', 'strand', and 'file_path'.\n",
        "\n",
        "  Returns:\n",
        "    A dict mapping organism to AlphaGenomeOutputMetadata.\n",
        "  \"\"\"\n",
        "  required_cols = {'file_path', 'name', 'output_type', 'strand'}\n",
        "  if not required_cols.issubset(track_metadata.columns):\n",
        "    raise ValueError(\n",
        "        'track_metadata must have columns %s. Missing: %s.',\n",
        "        required_cols,\n",
        "        required_cols - set(track_metadata.columns),\n",
        "    )\n",
        "  metadata = {}\n",
        "  for output_type, df_group in track_metadata.groupby('output_type'):\n",
        "    try:\n",
        "      output_type = dna_model.OutputType[str(output_type)]\n",
        "    except KeyError as e:\n",
        "      raise ValueError(f'Unknown output_type: {output_type}') from e\n",
        "    metadata[output_type.name.lower()] = df_group\n",
        "  return metadata_lib.AlphaGenomeOutputMetadata(**metadata)\n",
        "\n",
        "\n",
        "output_metadata = {\n",
        "    dna_model.Organism.HOMO_SAPIENS: build_output_metadata(TRACK_METADATA)\n",
        "}"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Data Pipeline\n",
        "\n",
        "Set up the training data iterator. This loads genomic intervals and\n",
        "corresponding track values from the specified BigWig files."
      ],
      "metadata": {
        "id": "3hwFXtTT59Rj"
      }
    },
    {
      "metadata": {
        "id": "a6_IlDTTUzjS"
      },
      "cell_type": "code",
      "source": [
        "ds_iter = finetune.get_dataset_iterator(\n",
        "    batch_size=BATCH_SIZE * jax.local_device_count(),\n",
        "    sequence_length=SEQUENCE_LENGTH,\n",
        "    output_metadata=output_metadata[ORGANISM],\n",
        "    organism=ORGANISM,\n",
        "    model_version=MODEL_VERSION,\n",
        "    subset=fold_intervals.Subset.TRAIN,\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "batch = next(ds_iter)\n",
        "pprint.pprint(jax.tree.map(np.shape, batch))"
      ],
      "metadata": {
        "id": "O69gDH0mx74N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Model Initialization\n",
        "\n",
        "Load the pretrained AlphaGenome checkpoint and initialize new output heads for\n",
        "the finetuning tracks."
      ],
      "metadata": {
        "id": "VYJe8QMd6XqX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "repo = f'google/alphagenome-{MODEL_VERSION.name.lower().replace('_', '-')}'\n",
        "checkpoint_path = huggingface_hub.snapshot_download(repo_id=repo)\n",
        "checkpointer = ocp.StandardCheckpointer()\n",
        "params_base, state_base = checkpointer.restore(checkpoint_path)"
      ],
      "metadata": {
        "id": "tHjm3TE11a_H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Set Up Device Mesh for Data Parallelism"
      ],
      "metadata": {
        "id": "_KReeM4A6hVR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_devices = jax.local_device_count()\n",
        "devices = mesh_utils.create_device_mesh((num_devices,))\n",
        "mesh = Mesh(devices, axis_names=('data',))\n",
        "data_sharding = P('data')\n",
        "replicated_sharding = P()"
      ],
      "metadata": {
        "id": "R9BWTLEyWe2X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initialize New Output Heads\n",
        "\n",
        "Create the forward function configured for our finetuning tracks and initialize\n",
        "the new head parameters."
      ],
      "metadata": {
        "id": "Zuq6_iYD6kgT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "forward_fn = finetune.get_forward_fn(output_metadata)\n",
        "with jax.set_mesh(mesh):\n",
        "  batch = jax.device_put(batch, data_sharding)\n",
        "  params_ft, state_ft = jax.jit(\n",
        "      forward_fn.init,\n",
        "      in_shardings=(replicated_sharding, data_sharding),\n",
        "      out_shardings=replicated_sharding,\n",
        "  )(jax.random.PRNGKey(0), batch)"
      ],
      "metadata": {
        "id": "Jma_onco3fkt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Merge Pretrained Trunk with New Heads\n",
        "\n",
        "Perform weight surgery: keep the pretrained trunk parameters and replace the\n",
        "head parameters with the newly initialized ones."
      ],
      "metadata": {
        "id": "vqNF4p7p6myu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "params_ft_head = hk.data_structures.filter(\n",
        "    lambda module_name, *_: 'head' in module_name, params_ft\n",
        ")\n",
        "params_base_no_head = hk.data_structures.filter(\n",
        "    lambda module_name, *_: 'head' not in module_name, params_base\n",
        ")\n",
        "params = hk.data_structures.merge(params_base_no_head, params_ft_head)\n",
        "state = state_base\n",
        "optimizer = optax.adam(LEARNING_RATE)\n",
        "opt_state = optimizer.init(params)\n",
        "train_step = jax.jit(\n",
        "    finetune.get_train_step(forward_fn.apply, optimizer),\n",
        "    in_shardings=(\n",
        "        replicated_sharding,\n",
        "        replicated_sharding,\n",
        "        replicated_sharding,\n",
        "        data_sharding,\n",
        "    ),\n",
        "    out_shardings=(\n",
        "        replicated_sharding,\n",
        "        replicated_sharding,\n",
        "        replicated_sharding,\n",
        "        replicated_sharding,\n",
        "    ),\n",
        ")"
      ],
      "metadata": {
        "id": "oht181gi5lLx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Training Loop\n",
        "\n",
        "Set up checkpointing and run the finetuning training loop."
      ],
      "metadata": {
        "id": "0fn9DT-_6sy-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Configure Checkpoint Directory"
      ],
      "metadata": {
        "id": "2a9Rj6DH6vrM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path_suffix = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "checkpoint_dir = epath.Path(SAVE_CHECKPOINT_DIR) / path_suffix\n",
        "checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
        "checkpoint_dir = str(checkpoint_dir)\n",
        "print('Saving to', checkpoint_dir)"
      ],
      "metadata": {
        "id": "fI2KOHT4icAu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpointer = ocp.StandardCheckpointer()\n",
        "\n",
        "\n",
        "def save(weights, idx):\n",
        "  ckpt_path = os.path.join(checkpoint_dir, 'checkpoint_{:05d}'.format(idx))\n",
        "  print(f'Saving checkpoint to {ckpt_path}')\n",
        "  checkpointer.save(ckpt_path, weights)\n",
        "  checkpointer.wait_until_finished()\n",
        "  return ckpt_path"
      ],
      "metadata": {
        "id": "jxOL3b9MZuk-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run Training"
      ],
      "metadata": {
        "id": "hz2g5VNU6yOd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss = []\n",
        "step = 0\n",
        "for step in range(NUM_TRAIN_STEPS):\n",
        "  try:\n",
        "    batch = next(ds_iter)\n",
        "  except StopIteration:\n",
        "    print('Dataset exhausted')\n",
        "    break\n",
        "  with jax.set_mesh(mesh):\n",
        "    batch = jax.device_put(batch, data_sharding)\n",
        "    params, state, opt_state, scalars = train_step(\n",
        "        params, state, opt_state, batch\n",
        "    )\n",
        "  loss.append(scalars['loss'])\n",
        "  if step % 10 == 1:\n",
        "    print('loss', step, loss[-1])\n",
        "ckpt_path = save((params, state), step + 1)"
      ],
      "metadata": {
        "id": "jLpIhWhY6NLC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Inference with Finetuned Model\n",
        "\n",
        "Load the finetuned checkpoint into a `DnaModel` for inference and compare\n",
        "predictions against ground truth."
      ],
      "metadata": {
        "id": "1n-T0v5Thy-o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load default organism settings but overwrite with fine-tuned output metadata.\n",
        "default_settings_human = dna_model.default_organism_settings()[\n",
        "    dna_model.Organism.HOMO_SAPIENS\n",
        "]\n",
        "settings_human_finetune = dataclasses.replace(\n",
        "    default_settings_human,\n",
        "    metadata=output_metadata[dna_model.Organism.HOMO_SAPIENS],\n",
        ")\n",
        "model = dna_model.create(\n",
        "    ckpt_path,\n",
        "    organism_settings={\n",
        "        dna_model.Organism.HOMO_SAPIENS: settings_human_finetune\n",
        "    },\n",
        ")"
      ],
      "metadata": {
        "id": "LXCHBEZ2Jgvn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Select Test Interval"
      ],
      "metadata": {
        "id": "PdxdX_aM7DHh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "interval = genome.Interval(\n",
        "    chromosome='chr21', start=46125238, end=46126738\n",
        ").resize(SEQUENCE_LENGTH)"
      ],
      "metadata": {
        "id": "GlZSwtrKe7F4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generate Predictions"
      ],
      "metadata": {
        "id": "knqkwu1a7IEl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preds = model.predict_interval(\n",
        "    interval,\n",
        "    requested_outputs=[dna_model.OutputType.RNA_SEQ],\n",
        "    ontology_terms=None,\n",
        ")"
      ],
      "metadata": {
        "id": "cV98xtYSeuu3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Ground Truth Tracks"
      ],
      "metadata": {
        "id": "wnb85sfp7K2F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "true_tracks = dataset_lib.MultiTrackExtractor(\n",
        "    output_metadata[ORGANISM], sequence_length=SEQUENCE_LENGTH\n",
        ").extract(interval)"
      ],
      "metadata": {
        "id": "UPhacPeDYCTN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualize Predictions vs Ground Truth"
      ],
      "metadata": {
        "id": "XR_mtt267o_J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compact_dict(**kwargs):\n",
        "  return {k: v for k, v in kwargs.items() if v is not None}\n",
        "\n",
        "\n",
        "def plot(*, interval, predictions, targets=None):\n",
        "  if targets is None:\n",
        "    colors = {'pred': 'black'}\n",
        "  else:\n",
        "    colors = {'pred': 'black', 'true': 'red'}\n",
        "  fig = plot_components.plot(\n",
        "      [\n",
        "          plot_components.OverlaidTracks(\n",
        "              tdata=compact_dict(\n",
        "                  pred=predictions.rna_seq,\n",
        "                  true=dataclasses.replace(\n",
        "                      predictions.rna_seq,\n",
        "                      values=targets['rna_seq'].astype(np.float32),\n",
        "                  )\n",
        "                  if targets is not None\n",
        "                  else None,\n",
        "              ),\n",
        "              colors=colors,\n",
        "          ),\n",
        "      ],\n",
        "      interval=interval.resize(int(2**11)),\n",
        "  )\n",
        "  return fig\n",
        "\n",
        "\n",
        "_ = plot(predictions=preds, interval=interval, targets=true_tracks)"
      ],
      "metadata": {
        "id": "drt2HPAubfcB"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "last_runtime": {}
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
